x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: Dockerfile
    args:
      AIRFLOW_HOME: ${AIRFLOW_HOME:-/opt/airflow}
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__DEFAULT_TIMEZONE: Asia/Seoul
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__LOGGING__BASE_LOG_FOLDER: ${AIRFLOW_HOME:-/opt/airflow}/logs
    AIRFLOW__LOGGING__LOGGING_LEVEL: INFO
    AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY:-ramdom-secret-key}
    AIRFLOW__LOGGING__WORKER_LOG_SERVER_PORT: 8793
    # AWS 자격증명
    AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
    AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-ap-northeast-2}
    AIRFLOW_CONN_AWS_DEFAULT: aws:///?aws_access_key_id=${AWS_ACCESS_KEY_ID}&aws_secret_access_key=${AWS_SECRET_ACCESS_KEY}&region_name=ap-northeast-2&read_timeout=900
    # Airflow Variables
    AIRFLOW_VAR_S3_BUCKET: ${AIRFLOW_VAR_S3_BUCKET}
    AIRFLOW_VAR_S3_CONFIG_BUCKET: ${AIRFLOW_VAR_S3_CONFIG_BUCKET}
    AIRFLOW_VAR_BATCH_INTERVAL_MINUTES: ${AIRFLOW_VAR_BATCH_INTERVAL_MINUTES:-180}
    AIRFLOW_VAR_BATCH_DURATION_HOURS: ${AIRFLOW_VAR_BATCH_DURATION_HOURS:-24}
    # EMR Variables
    AIRFLOW_VAR_EMR_BASE_LOG_URI: ${AIRFLOW_VAR_EMR_BASE_LOG_URI}
    AIRFLOW_VAR_EMR_SERVICE_ROLE: ${AIRFLOW_VAR_EMR_SERVICE_ROLE}
    AIRFLOW_VAR_EMR_EC2_ROLE: ${AIRFLOW_VAR_EMR_EC2_ROLE}
    AIRFLOW_VAR_EMR_SUBNET_ID: ${AIRFLOW_VAR_EMR_SUBNET_ID}
    AIRFLOW_VAR_EMR_KEY_PAIR: ${AIRFLOW_VAR_EMR_KEY_PAIR}
    AIRFLOW_VAR_EMR_MASTER_SG: ${AIRFLOW_VAR_EMR_MASTER_SG}
    AIRFLOW_VAR_EMR_SLAVE_SG: ${AIRFLOW_VAR_EMR_SLAVE_SG}
    AIRFLOW_VAR_EMR_BOOTSTRAP_SCRIPT_PATH: ${AIRFLOW_VAR_EMR_BOOTSTRAP_SCRIPT_PATH}
    AIRFLOW_VAR_EMR_OPENAI_API_KEY: ${AIRFLOW_VAR_EMR_OPENAI_API_KEY}
    AIRFLOW_VAR_EMR_SCRIPT_PATH: ${AIRFLOW_VAR_EMR_SCRIPT_PATH}
    # Redshift Variables
    AIRFLOW_VAR_REDSHIFT_DATABASE: ${AIRFLOW_VAR_REDSHIFT_DATABASE}
    AIRFLOW_VAR_REDSHIFT_SCHEMA: ${AIRFLOW_VAR_REDSHIFT_SCHEMA}
    AIRFLOW_VAR_REDSHIFT_TABLE: ${AIRFLOW_VAR_REDSHIFT_TABLE}
    AIRFLOW_VAR_S3_SOURCE_PATH: ${AIRFLOW_VAR_S3_SOURCE_PATH}
    AIRFLOW_VAR_REDSHIFT_IAM_ROLE: ${AIRFLOW_VAR_REDSHIFT_IAM_ROLE}
    AIRFLOW_VAR_REDSHIFT_WORKGROUP: ${AIRFLOW_VAR_REDSHIFT_WORKGROUP}
    AIRFLOW_VAR_REDSHIFT_SECRET_ARN: ${AIRFLOW_VAR_REDSHIFT_SECRET_ARN}

  volumes:
    - ./bucket/dags:${AIRFLOW_HOME:-/opt/airflow}/dags
    - ./bucket/configs:${AIRFLOW_HOME:-/opt/airflow}/configs
  depends_on: &airflow-common-depends-on
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: always

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command: -c "airflow db init && airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.com"
    depends_on:
      postgres:
        condition: service_healthy

  airflow-webserver:
    <<: *airflow-common
    command: airflow webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always

  airflow-scheduler:
    <<: *airflow-common
    command: airflow scheduler
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always